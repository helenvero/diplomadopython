{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset_train = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "dataset_test = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset_test, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64], output_size=10, activation_fn=nn.ReLU):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(activation_fn())\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(activation_fn())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoca [{epoch + 1}/{epochs}], Perdida: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Exactitud: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "experimentos = [\n",
    "    {\"hidden_sizes\": [128, 64], \"activation_fn\": nn.ReLU, \"lr\": 0.01, \"batch_size\": 64, \"epochs\": 10},\n",
    "    {\"hidden_sizes\": [256, 128], \"activation_fn\": nn.Tanh, \"lr\": 0.001, \"batch_size\": 32, \"epochs\": 10},\n",
    "    {\"hidden_sizes\": [512, 256, 128], \"activation_fn\": nn.ReLU, \"lr\": 0.005, \"batch_size\": 128, \"epochs\": 15},\n",
    "]\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for i, config in enumerate(experimentos):\n",
    "    print(f\"\\Experimento {i + 1}\")\n",
    "    model = MLP(hidden_sizes=config[\"hidden_sizes\"], activation_fn=config[\"activation_fn\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "\n",
    "    train_model(model, train_loader, criterion, optimizer, epochs=config[\"epochs\"])\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    resultados.append({\"config\": config, \"accuracy\": accuracy})\n",
    "\n",
    "print(\"\\nResumen de experimentos\")\n",
    "for i, resultado in enumerate(resultados):\n",
    "    print(f\"Experimento {i + 1}: Exactitud = {resultado['accuracy'] * 100:.2f}%, Config = {resultado['config']}\")\n",
    "\n",
    "mejor_experimento = max(resultados, key=lambda x: x[\"accuracy\"])\n",
    "print(f\"\\nMejor Modelo: Exactitud = {mejor_experimento['accuracy'] * 100:.2f}%, Config = {mejor_experimento['config']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_filters=[32, 64], fc_neurons=128, dropout=0.5):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filters[0], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(num_filters[0], num_filters[1], kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(num_filters[1] * 7 * 7, fc_neurons),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fc_neurons, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, epocas=10):\n",
    "    model.train()\n",
    "    for epoca in range(epocas):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoca [{epoca + 1}/{epocas}], Perdida: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    print(f\"Exactitud: {accuracy * 100:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "experiments = [\n",
    "    {\"num_filters\": [32, 64], \"fc_neurons\": 128, \"dropout\": 0.5, \"lr\": 0.01, \"epocas\": 10},\n",
    "    {\"num_filters\": [64, 128], \"fc_neurons\": 256, \"dropout\": 0.3, \"lr\": 0.001, \"epocas\": 10},\n",
    "    {\"num_filters\": [16, 32], \"fc_neurons\": 64, \"dropout\": 0.2, \"lr\": 0.005, \"epocas\": 15},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(experiments):\n",
    "    print(f\"\\nExperimento {i + 1}\")\n",
    "    model = CNN(num_filters=config[\"num_filters\"], fc_neurons=config[\"fc_neurons\"], dropout=config[\"dropout\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    start_time = time.time()  # Usando time después de importarlo\n",
    "    train_model(model, train_loader, criterion, optimizer, epocas=config[\"epocas\"])\n",
    "    elapsed_time = time.time() - start_time  # Tiempo transcurrido\n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    results.append({\"config\": config, \"accuracy\": accuracy, \"time\": elapsed_time})\n",
    "\n",
    "# Reporte final\n",
    "print(\"\\nSuma de Experimentos\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Experimiento {i + 1}: Exactitud = {result['accuracy'] * 100:.2f}%, Tiempo = {result['time']:.2f}s, Config = {result['config']}\")\n",
    "\n",
    "# Selección del mejor modelo\n",
    "best_experiment = max(results, key=lambda x: x[\"accuracy\"])\n",
    "print(f\"\\nMejor Modelo: Exactitud = {best_experiment['accuracy'] * 100:.2f}%, Config = {best_experiment['config']}\")\n",
    "\n",
    "# Comparación final con el MLP\n",
    "print(\"\\nComparacion con MLP\")\n",
    "mlp_time = 15.0  # Tiempo estimado del MLP, ajustar según los resultados\n",
    "mlp_accuracy = 0.95  # Accuracy estimado del MLP, ajustar según los resultados\n",
    "\n",
    "print(f\"MLP - Exactitud: {mlp_accuracy * 100:.2f}%, Time: {mlp_time:.2f}s\")\n",
    "print(f\"Mejor CNN - Exactitud: {best_experiment['accuracy'] * 100:.2f}%, Tiempo: {best_experiment['time']:.2f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
